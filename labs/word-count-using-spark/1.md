# <center>Execute WordCount Spark program from Hadoop Platform.</center>

# **Introduction about Spark program.**
Spark is a formally defined computer programming language based on the Ada programming language, intended for the development of high integrity software used in systems where predictable and highly reliable operation is essential. It facilitates the development of applications that demand safety, security, or business integrity.
Spark is popular because of :
  a) Ogical soundness
  b) Rigorous formal definition
  c) Simple semantics
  d) Security
  e) Expressive power
  f) Verifiability
  g) Bounded resource (space and time) requirements.
  h) Minimal runtime system requirements

https://en.wikipedia.org/wiki/SPARK_(programming_language)


User can learn Spark programming language by writing code on a Cloud base IDE on top of Hadoop based DLP environment. Execution process and viewing outout of a sample WordCount Spark program is described here to learn and understand the process of using IDE. User can get familier about Hadoop environment also.

## Lab Overview

WordCount is a classic example of learning Spark programming language. From our Data Learning Platform(DLP), user can  write code using Spark programming language, execute the program and view the output on Hadoop platform. Main aim of this platform is to use Hadoop environment for writing and executeing Spark programming language and view the output using data files directly from the platform.

After completing this lab exercise, user can try their own spark program using different data file from Hadoop environment.

<font color='red'>Request access to the Data Learning Platform by sending a message to:</font> [datalearningplatform@cisco.com](mailto:datalearningplatform@cisco.com)

## Lab Objectives

* Learn how to get data from a file from Hadoop environment.
* Learn how to use some RDD operations on data using Spark.
* Learn how to write data to a new file with file creation in Hadoop.

## Prerequisites

* Knowledge on Hadoop to store the data.
* Basic knowledge of how spark works.
* Chrome Browser.

## Step 1: Explore Data Learning Platform(DLP)
DLP platform is pre-populated with workspace for executing different programing language. For using the Spark programming language, user need to select the below workspace.

1)	After logging on DLP, click on <b>Development Hub</b> on the left column.<br>
2)	Select the pre-defined work space named as <b>wksp-scala</b>.<br>
3)	Click on the <b>Launch</b> button to open IDE workspace.<br>

![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/SelectWorkSpace.PNG?raw=true)

## Step 2: Explore IDE Workspace
For writing any programming language, DLP platform is pre-loaded with an IDE. User can write Spark programing language in this IDE. </br>
Below sections will help to explore the IDE for Word Count project(wordcount). In the IDE, files are listed in Left Panel.
Double click on WordCount.scala file. Left Panel will be populated with the code of this file. 

![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/WordCountScalafile.PNG?raw=true)


<b> Pre-loaded raw data file named(wordcountinputfile1.txt) in HDFS environment.</b><br>
This program will count the number of words in the file by identifying spaces between words. This file is already pre-loaded in hadoop environment. 
N.B: We use a sample file with small data set(wordcountinputfile1.txt) containing pre-populated words pre-loaded in Hadoop. Spark program will exeecute on this sample data file and execute the occurence of each word and show the output.

``` json
//Import libraries which are needed to run the program. 
import org.apache.spark.{SparkContext, SparkConf}
object WordCount
{
  def main(args: Array[String]) {
    val inputFile = args(0)
    val outputFile = args(1)
    val conf = new SparkConf().setAppName("wordCount")
    // Create a Scala Spark Context.
    val sc = new SparkContext(conf)
    // Load our input data.
    val input = sc.textFile(inputFile)
    // Split up into words.
    val words = input.flatMap(line => line.split(" "))
    // Transform into word and count.
    val counts = words.map(word => (word, 1)).reduceByKey{_ + _}
    // Save the word count back out to a text file, causing evaluation.
    counts.saveAsTextFile(outputFile)
  }
}
```
N.B: Use WordCount.scala to view the above code. 

# Step 3 :  Configuring and Executing Spark Program from DLP's IDE.
We are going to learn the process of setting configurations and using IDE to execute Spark Program. Our Sample Program will count the occurance of each word from a pre-loaded text file with multiple words which is already saved in DLP's Hadoop environment </br>
Pre-loaded sample data file is loaded in belowlocation: </br>
<b>Input file: hdfs://172.16.1.11:8020/tmp/spark/input/wordcountinputfile1.txt</b>

<b> (3.1) Setting Configuration files for WordCount Program. </b> </br>
Configuration details are saved in a shell script file mentioned in below. User need to set the parameters there to getting proper output. Details of each parameters are also described in below. </br>
1) Select the WordCountRun.sh file from left panel of the IDE and double click on it. </br>
This step will show the process of Running the Spark program from the IDE. Program will be compiled from Hadoop platfrom.</br>

![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/STEP3.png?raw=true)

</br>
2) There are two parameters which we need to set before we start compiling the Spark program. <br>
a) Input Parameter: Here, input file from hdfs path is "wordcountinputfile1.txt"</br>
<b>N.B:</b> For this sample program, the input file is pre-loaded in Hadoop environment.  User need not change this file path.</br>
b) Output Parameter: Here, output file which would be saved in hdfs. </br>
   export OUTPUT = <Enter output file name> </br>
<b>N.B:</b> User need to enter a valid text as output file name. For example, output47 would be the name of output file name. </br>

<br>
  Input file: hdfs://172.16.1.11:8020/tmp/spark/input/wordcountinputfile1.txt.
  Output file: export OUTPUT=output47<br>
  <b>Change the output file name(e.g: <b>output47</b>) based on your choice. This name should be unique.</b>
</br>

<b> (3.2) Build and Execute Spark Program from DLP's IDE </b></br>
In this step, User will learn the process of packaging tne building the program from DLP's Cloud IDE. User can use the sample word count program to package and build. This packaging and build process will work on Hadoop environment.
1) Double click on WordCound.scala file again. Change <b>"package"</b> as described in the below image and click on the <b>"run"</b> blue button. This process will build and package the program.
![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/buildWordCount.PNG?raw=true)

2) You can find the below screen shot if build process finished successfully.
![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/buildSuccessWordCount.PNG?raw=true)

3) If the build process finished successfully, then select <b>"run"</b> as described in the below image and click on <b>blue run</b> button. 
![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/runWordCount.PNG?raw=true)

4) Successful run process will show the output as below. (There should NOT be any Exception message in the exception window if run process finished successfully.)
![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/successfulBuildProcess.png?raw=true)

<b> (3.3) Displaying output from DLP's IDE.</b></br>
Above program will save the output in Hadoop environment. Below steps is helpful to view that output.

1) After completing above steps, open the <b>view.sh</b> by double clicking on it. Change the OUTPUT parameter for the output file name which is defined before. In our sample application, we define it as output47. After changing the output parameter, select <b>view</b> and click on the <b>blue run</b>
Output parameter is : export OUTPUT = output47
<b>Here value of OUPUT parameter should be same as described in Step 3.1.2.b<b></br>

![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/viewOutput.png?raw=true)

This will show the number of occurance of each word in the output window. <b>view.sh</b> file is used to pickup the file from Hadoop environment and show the output in IDE's output window. 


Here Input file has the text: <b>welcome to data data learning platform cisco</b>


Things to Try:

* Try with numeric data type
* Try with case sensitive data as well.

Completing this coding exercise, we have learned how to count the number of words in an input file using Spark Batch Processing. <br>

There are some more examples and exercise are available in the below mentioned link. This is for your reference.
[http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html).
