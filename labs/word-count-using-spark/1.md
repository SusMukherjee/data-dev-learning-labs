# <center>Execute WordCount Scala program using Apache Spark from Hadoop Platform.</center>

# **Introduction about Spark program.**
Apache Spark is an open source cluster computing framework. Spark provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance. Apache Spark provides programmers with an application programming interface centered on a data structure called the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. It was developed in response to limitations in the MapReduce cluster computing paradigm, which forces a particular linear dataflow structure on distributed programs: MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk. Spark's RDDs function as a working set for distributed programs that offers a (deliberately) restricted form of distributed shared memory.

https://en.wikipedia.org/wiki/Apache_Spark

User can learn Scala programming language using Spark by writing code on a Cloud base IDE on top of Hadoop based DLP environment. Execution process and viewing output of a sample WordCount Spark program is described here to learn and understand the process of executing program by using the cloud IDE. User can get familier about Hadoop environment also.

## Lab Overview

WordCount is a classic example of learning Scala programming language. From our Data Learning Platform(DLP), user can  write code using Scala language, execute the program and view the output on Hadoop platform. Main aim of this platform is to use Hadoop environment for writing and executeing Spark and view the output using data files directly from the platform.

After completing this lab exercise, user can try their own spark program using different data file from Hadoop environment.

<font color='red'>Request access to the Data Learning Platform by sending a message to:</font> [datalearningplatform@cisco.com](mailto:datalearningplatform@cisco.com)

## Lab Objectives

* Learn how to get data from a file from Hadoop environment.
* Learn how to use some RDD operations on data using Spark.
* Learn how to write data to a new file with file creation in Hadoop.

## Prerequisites

* Knowledge on Hadoop to store the data.
* Basic knowledge of how spark works.
* Chrome Browser.

## Step 1: Explore Data Learning Platform(DLP)
DLP platform is pre-populated with workspace for executing different programing language. For using the Spark programming language, user need to select the below workspace.

1)	After logging on DLP, click on <b>Development Hub</b> on the left column.<br>
2)	Select the pre-defined work space named as <b>wksp-wordcount</b>.<br>
3)	Click on the <b>Launch</b> button to open IDE workspace.<br>

![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/SelectWorkSpace.PNG?raw=true)

## Step 2: Explore IDE Workspace
For writing any programming language, DLP platform is pre-loaded with an IDE. User can write Spark programing language in this IDE. </br>
In the IDE, files are listed in Left Panel. Double click on WordCount.scala file. Right Panel will be populated with the code of this file. </br>
*Except the file names specified in this document, are required for this learning lab. Others files should be used for another learning lab which are not related to this learning lab.* </br>

![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/WordCountScalafile.PNG?raw=true)


``` json
//Import libraries which are needed to run the program. 
import org.apache.spark.{SparkContext, SparkConf}
object WordCount
{
  def main(args: Array[String]) {
    val inputFile = args(0)
    val outputFile = args(1)
    val conf = new SparkConf().setAppName("wordCount")
    // Create a Scala Spark Context.
    val sc = new SparkContext(conf)
    // Load our input data.
    val input = sc.textFile(inputFile)
    // Split up into words.
    val words = input.flatMap(line => line.split(" "))
    // Transform into word and count.
    val counts = words.map(word => (word, 1)).reduceByKey{_ + _}
    // Save the word count back out to a text file, causing evaluation.
    counts.saveAsTextFile(outputFile)
  }
}
```
N.B: Use WordCount.scala to view the above code. 
In the above, Scala code using Spark is using to read the input file (passed as a parameter), generate the Spark context, Read the input file and split words using a single space and increment the counter of each occurance of word and write the output in console.

**Edit input file name with a sentence.** </br>
Double click on wordcountinputfiles2.txt from left panel. This file name may be changed for differnet user. Input some content there.
![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/EditInputFile.png?raw=true)

<b> (3.2) Build and Execute Spark Program from DLP's IDE </b></br>
In this step, User will learn the process of packaging and building the program from DLP's Cloud IDE. User can use the sample word count program to package and build. This packaging and build process will work on Hadoop environment.
1) Double click on WordCound.scala file again. Change <b>"package"</b> as described in the below image and click on the <b>"run"</b> blue button. This process will build and package the program.
![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/buildWordCount.PNG?raw=true)

2) You can find the below screen shot if build process finished successfully.
![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/buildSuccessWordCount.PNG?raw=true)

3) If the build process finished successfully, then select <b>"run"</b> as described in the below image and click on <b>blue run</b> button again. 
![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/runWordCount.PNG?raw=true)

4) Successful run process will show the output as below. (There should NOT be any Exception message in the exception window if run process finished successfully.)
![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/successfulBuildProcess.png?raw=true)

<b> (3.3) Displaying output from DLP's IDE.</b></br>
Above program will save the output in Hadoop environment. Below steps is helpful to view that output.

1) After completing above steps, open the <b>view.sh</b> by double clicking on it. Select <b>view</b> and click on the <b>blue run</b> button to view the output.

![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/viewOutput.png?raw=true)

This will show the number of occurance of each word in the output window. <b>view.sh</b> file is used to pickup the file from Hadoop environment and show the output in IDE's output window. 


Here Input file has the text: <b>Hello spark I love Cisco Company. Hello all. We are practicing Word Count Program using Scala and Spark.</b>
Output content is as below:
```json
(are,1)
(Program,1)
(Hello,2)
(love,1)
(Word,1)
(practicing,1)
(using,1)
(We,1)
(Scala,1)
(spark,1)
(Count,1)
(I,1)
(Company.,1)
(Spark.,1)
(Cisco,1)
(and,1)
(all.,1)
```

Things to Try:

* Try with numeric data type
* Try with case sensitive data as well.

Completing this coding exercise, we have learned how to count the number of words in an input file using Spark Batch Processing. <br>

There are some more examples and exercise are available in the below mentioned link. This is for your reference.
[http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html).
